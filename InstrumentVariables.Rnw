% !Rnw root = panel-data-R-ref.Rnw

\chapter{Instrument Variables}
\label{Chapt:IV}

In the previous chapters we assumed that our linear model was exogenous. We now consider what happens when that 
assumption is relaxed. But first we give a definition:

\begin{definition}{Endogenous}
The explanatory term $Xn_{i,t}$ is said to be {\it endogenous} provided it is correltated with $\sigma_{i,t}$ , that is $\rho(X_{i,t}, \epsilon_{i,t})\neq 0$.
\end{definition}


\section{Bias Arrising From Endogenity of X}
In the the previous sections we made the the assumption that the error term was uncorrelated with the explanatory variables. One way to visualize this is

\begin{center}
\begin{tikzpicture}
  \node (X) {$X$};
  \node (E) [below of=X] {$\epsilon$};
  \node (Y) [right of=X] {$Y$};
  \draw[->] (X) to node {} (Y);
  %\draw[->, dashed] (X) to node [swap] {} (E);
  \draw[->] (E) to node [swap] {} (Y);
\end{tikzpicture}
\end{center}
Here the arrows illustrate Y dependence on $X$ and $\epsilon$.

Now the assumption that $X$ and $\epsilon$ are uncorrelate may not in practice be true, i.e. we may find ourselves in the situation where
\begin{equation}
  \rho(X_{i,t},\epsilon_{i,t}) \neq 0
\end{equation}
We can illustrate the correlation between $X$ and $\epsilon$ by connection them with a line as follows:
%\begin{figure}
\begin{center}
\begin{tikzpicture}
  \node (X) {$X$};
  \node (E) [below of=X] {$\epsilon$};
  \node (Y) [right of=X] {$Y$};
  \draw[->] (X) to node {} (Y);
  \draw[-, dashed] (E) to node [swap] {} (X);
  \draw[->] (E) to node [swap] {} (Y);
\end{tikzpicture}
%\end{figure}
\end{center}

In this case, when $X$ and $\epsilon$ are correlated the  OLS estimator becomes {\it biased}.
This is easily illustrated by considering the following simple example:
Let 
\begin{equation}
Y_i= \beta X_i + \epsilon
\end{equation} 
Now since, $\hat \beta = [X^TX]^{-1} x^T Y$ we see
\begin{equation}
 \hat \beta \rightarrow 
 \frac{E(XY)}{E(XX)} =
 \frac{E(X(\beta X+\epsilon))}{E(X^2)}=
 %\beta \frac{E(X^2)}{E(X^2)} + \frac{E(X\epsilon)}{E(X^2)} = 
 \beta + \frac{E(X\epsilon)}{E(X^2)}
\end{equation}
Now recall
\begin{equation}
\sigma_X^2=E(X^2)-\mu_X^2
\end{equation}
and 
\begin{equation}
\rho_{X\epsilon}=\frac{E(X \epsilon) -\mu_X \mu_\epsilon}{\sigma_X \sigma_\epsilon}
\end{equation}
So 
\begin{equation}
\frac{E(X\epsilon)}{E(X^2)}=
\frac{\sigma_X \sigma_\epsilon \rho_{x \epsilon} + \mu_X \mu_\epsilon}
{\sigma_X^2 + \sigma_\epsilon^2}
\end{equation}
Now since $\mu_\epsilon=0$ the last term drops out so we get
\begin{equation}
\hat \beta \rightarrow
\beta + \frac{\sigma_X \sigma_\epsilon \rho_{x \epsilon} }
{\sigma_X^2 + \sigma_\epsilon^2}
\end{equation}
Now since $\sigma_X \neq 0$ 
\footnote{there is more that one X value} and $\sigma_\epsilon \neq 0$ 
we see that the necessary and sufficient condition for $\hat \beta \rightarrow \beta$ is 
that $\rho_{x \epsilon} \neq 0$.

\section{Two Stage Least Squares}
Thus, when $X$ and $\epsilon$ are correlated, we are tempted find a work around in order to avoid bias. 

One such work around is to have another variable, say $Z$, which is correlated with $X$ but not correlated with $\epsilon$. Such a variable $Z$ is called an {\it Instrument Variable}. 

We may visualize this as

\begin{center}
\begin{tikzpicture}
  \node (X) {$X$};
  \node (E) [below of=X] {$\epsilon$};
  \node (Z) [left of=X]{$Z$};
  \node (Y) [right of=X] {$Y$};
  \draw[->] (X) to node {} (Y);
  \draw[->] (Z) to node {} (X);
  \draw[-, dashed] (E) to node [swap] {} (X);
  \draw[->] (E) to node [swap] {} (Y);
\end{tikzpicture}
\end{center}

An instrument variable can allow us to avoid bias by replacing the role of $X$ in $Y=X \beta + \epsilon$ by a proxy $\tilde X$. 
More precisely, first regress on
\begin{equation}
X=Z \alpha +\eta
\end{equation} 
 to obtain $\hat \alpha$
 Next compute the predicted values 
 \begin{equation}
 \tilde X = Z \hat \alpha
 \end{equation} 
 Finally compute $\hat \beta$ by regressing on 
 \begin{equation}
 Y = \tilde X \beta + \epsilon
 \end{equation}
Since $Z$ and $\epsilon$ are uncorrelated, $\tilde X$ and $\epsilon$ are also uncorrelated. 

\section{Combining the 2 Stage Regression Calculations}

In the previous section we used instrument variables to perform a two stage regression. Here will we examine the details and combine to provide a single equivalent calculation.

Now the OLS solution of $X=Z \alpha + \eta$ is given by
\begin{equation}
\hat \alpha = [Z^TZ]^{-1}Z^TX
\end{equation}
so $\tilde X$ is given by
\begin{equation}
\tilde X = Z\hat \alpha = Z[Z^TZ]^{-1}Z^TX
\end{equation}
For notational convenience, define P by
%set $P=Z[Z^TZ]^{-1}Z^T$. Note P is symmetric ($P=P^T$) and a projection ($P^2=P$)
\begin{equation}
P=Z[Z^TZ]^{-1}Z^T
\end{equation}
Then 
\begin{equation}
\tilde X = PX
\end{equation}
and $\hat \beta$ is given by
\begin{equation}
\hat \beta = [(PX)^T(PX)]^{-1}(PX)^TY
\end{equation}
So
\begin{equation}
\hat \beta = [X^TP^TPX]^{-1}X^TP^TY
\end{equation}
This can be simplifed further by noting that P is symmetric,
\footnote{ This follows since $P^T= 
\left[ Z[Z^TZ]^{-1}Z^T \right]^T =
[Z^T]^T [[Z^TZ]^{-1}]^T [Z]^T =
Z [[Z^TZ]^{T}]^{-1} Z^T = 
Z [Z^TZ]^{-1} Z^T.$ Thus $P^T =
P$, so $P$ is symmetric
}
to get
\begin{equation}
\hat \beta = [X^TP^2X]^{-1}X^TPY
\end{equation}
Furthermore, we note P is a projection,
\footnote{ To see P is a projection, note
$P^2=
\left[ Z[Z^TZ]^{-1}Z^T \right]\left[ Z[Z^TZ]^{-1}Z^T \right]=
Z[Z^TZ]^{-1}Z^T Z[Z^TZ]^{-1}Z^T=
Z[Z^TZ]^{-1}Z^T =P$
}
thus
\begin{equation} \label{eq:IV.general}
\hat \beta = [X^TPX]^{-1}X^TPY
\end{equation}
Now eq~\ref{eq:IV.general} is the general form for the solution using instrument variables. 


In the event that the number of instruments is equal to the number of explanatory variables, the term $Z^T X$ becomes a  square matrix. In this case, it makes sense to speak of $[Z^TX]^{-1}$ and to recast eq~\ref{eq:IV.general} as follows:
% \begin{equation}
% \begin{array}{lcl}
% \hat \beta & = & [X^TPX]^{-1} X^T P Y \\
%             & = &[X^TPX]^{-1} X^T P X Z^T (XZ^T)^{-1}Y \\
%            & = & Z^T (XZ^T)^{-1} Y \\
%            & = & (Z^T X)^{-1} Z^T X Z^T (X Z^T)^{-1} Y \\
%            & = & (Z^T X)^{-1} Z^T  Y          
% \end{array}
% \end{equation}
Insert $X Z^T (XZ^T)^{-1}$ in front of the Y of eq~\ref{eq:IV.general} to get
\begin{equation}
\hat \beta 
 = [X^TPX]^{-1} X^T P X Z^T (XZ^T)^{-1}Y 
\end{equation}
Since the product of  $[X^TPX]^{-1}$ and  $[X^T P X Z^T]$ is the identity,
\begin{equation}
\hat \beta 
 = Z^T (XZ^T)^{-1} Y
\end{equation}
Now premultiply $Z^T (XZ^T)^{-1} Y$ by the identity $(Z^T X)^{-1} Z^T X$ to get
\begin{equation}
\hat \beta 
 = (Z^T X)^{-1} Z^T X Z^T (X Z^T)^{-1} Y
\end{equation}
Reducing the $X Z^T (X Z^T)^{-1}$ term we finally get
\begin{equation}
\hat \beta 
 = (Z^T X)^{-1} Z^T  Y
\end{equation}

\section{Using Instrument Variables in R}

Using the {\it plm package} we may incorporate instrument variables using the "$\vert$"  symbol followed by a list of instruments.  As an example,
consider the data generated in Appendix ??? . 

<<>>=
df=read.csv( "./Data/InstrumentVbl.csv")
res<-plm(Y~X|Z, data=df, index="Date", model="pooling")
summary(res)
@




We may may compare this to the model without the instrument variable $Z$:

<<>>=
df=read.csv( "./Data/InstrumentVbl.csv")
res2<-plm(Y~X, data=df, index="Date", model="pooling")
summary(res2)
@

Now the data in the panel "InstrumentVbl.csv" was generated using a value of $\beta=3$
\footnote{See Appendix~\ref{appendix::Syn::IV}}, we can ascertain the goodness of our estimates.  

